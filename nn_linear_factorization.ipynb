{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ae1808",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a567509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:11.555849Z",
     "start_time": "2021-11-01T13:09:10.419588Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd5cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:13.795512Z",
     "start_time": "2021-11-01T13:09:11.558134Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8225f50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:13.841075Z",
     "start_time": "2021-11-01T13:09:13.797639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.21.2\n",
      "torch: 1.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -i -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040643c3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "## Setup\n",
    "From [\"Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer\"](https://arxiv.org/abs/1910.13923):\n",
    "\n",
    "\"The design is based on matrix factorization by approximating the matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ in the linear feed-forward unit using two smaller matrices, $\\mathbf{E} \\in \\mathbb{R}^{m \\times r}$ and $\\mathbf{D} \\in \\mathbb{R}^{r \\times n}$:\n",
    "\n",
    "$\\mathbf{W} \\approx \\mathbf{E} \\times \\mathbf{D}$\n",
    "\n",
    "The matrix $\\mathbf{W}$ requires $m n$ parameters and $m n$ flops, while $\\mathbf{E}$ and $\\mathbf{D}$ require $r m+r n=r(m+n)$ parameters and $r(m+n)$ flops.\n",
    "\n",
    "If we take the rank to be very low $r<<m, n$, the number of parameters and flops in $\\mathbf{E}$ and $\\mathbf{D}$ are much smaller compared to $\\mathbf{W}$.\"\n",
    "\n",
    "## Applications\n",
    "This setup is interesting because it could potentially reduce the memory for Transformer setups using short-sequence lengths, as outlined in  [\"Greenformers - Improving Computation and Memory Efficiency in Transformer Models via Low-Rank Approximation\"](https://arxiv.org/abs/2108.10808):\n",
    "\n",
    "\"The Low-Rank Transformer model is suitable for improving both the time and memory efficiency in processing short-sequence (â‰¤ 512) input data, while the Linformer model is suitable for improving the efficiency in processing long-sequence input data (> 512).\"\n",
    "\n",
    "**$\\rightarrow$ This setup can be very interesting for Transformer setups that usually have sequence lengths below 768, e.g., ViT!**\n",
    "\n",
    "In addition, it does not suffer from the two deficiencies of the Linformer setup (see https://github.com/lucidrains/linformer#linformer-for-pytorch) as it works for the auto-regressive case and assumes no fixed sequence length.\n",
    "\n",
    "## Sources\n",
    "The outlined \"linear factorization\" setup is inspired by:\n",
    "1. https://discuss.pytorch.org/t/factorization-of-a-weight-matrix-as-products-of-low-rank-matrices/76278\n",
    "1. [\"Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer\"](https://arxiv.org/abs/1910.13923)\n",
    "1. [\"Greenformers - Improving Computation and Memory Efficiency in Transformer Models via Low-Rank Approximation\"](https://arxiv.org/abs/2108.10808)\n",
    "1. https://github.com/lucidrains/linformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7bafe",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d11d6f",
   "metadata": {},
   "source": [
    "Code setup inspired by https://discuss.pytorch.org/t/how-to-replace-all-relu-activations-in-a-pretrained-network/31591/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c01643d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:13.890927Z",
     "start_time": "2021-11-01T13:09:13.845635Z"
    }
   },
   "outputs": [],
   "source": [
    "def nn_linear_factorization(model, rank=64):\n",
    "    \"\"\"\n",
    "    Recursively replace nn.Linear(in_features=a, out_features=b) with\n",
    "    nn.Sequential(nn.Linear(a, rank),\n",
    "                  nn.Linear(rank, b)))\n",
    "    to replace the big linear layer as a factorization consisting of\n",
    "    a sequence of two smaller linear layers.\n",
    "    \n",
    "    The lower the rank hyperparameter is, the higher the memory savings can be.\n",
    "    \"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(model, name, nn.Sequential(nn.Linear(child.in_features, rank),\n",
    "                                               nn.Linear(rank, child.out_features)))\n",
    "        else:\n",
    "            nn_linear_factorization(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba8dc1",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a6505e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:13.930461Z",
     "start_time": "2021-11-01T13:09:13.892349Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_parameter_count(model):\n",
    "    return np.sum([np.prod(p.shape) for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91e4df",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168ede7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:13.968695Z",
     "start_time": "2021-11-01T13:09:13.931764Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_simple_model():\n",
    "    return nn.Sequential(nn.Linear(768, 256), nn.Linear(256, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2fc530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.004197Z",
     "start_time": "2021-11-01T13:09:13.970425Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183e5125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.039278Z",
     "start_time": "2021-11-01T13:09:14.005580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (1): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d93d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.073985Z",
     "start_time": "2021-11-01T13:09:14.040779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_orig = get_parameter_count(model); params_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bfa51c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.117452Z",
     "start_time": "2021-11-01T13:09:14.077493Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_linear_factorization(model, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc39a046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.150934Z",
     "start_time": "2021-11-01T13:09:14.119145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64126bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.191681Z",
     "start_time": "2021-11-01T13:09:14.152297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90624"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_factorized = get_parameter_count(model); params_factorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "494a571d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.230320Z",
     "start_time": "2021-11-01T13:09:14.193021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229760, 90624, 0.39)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_orig, params_factorized, round(params_factorized/params_orig, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da3b26",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa82ea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T12:56:29.269559Z",
     "start_time": "2021-11-01T12:56:29.210092Z"
    }
   },
   "source": [
    "Based on: https://github.com/lucidrains/x-transformers#usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086f2238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.283275Z",
     "start_time": "2021-11-01T13:09:14.231763Z"
    }
   },
   "outputs": [],
   "source": [
    "from x_transformers import XTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc53c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.598206Z",
     "start_time": "2021-11-01T13:09:14.285053Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XTransformer(\n",
    "    dim = 512,\n",
    "    enc_num_tokens = 256,\n",
    "    enc_depth = 6,\n",
    "    enc_heads = 8,\n",
    "    enc_max_seq_len = 1024,\n",
    "    dec_num_tokens = 256,\n",
    "    dec_depth = 6,\n",
    "    dec_heads = 8,\n",
    "    dec_max_seq_len = 1024,\n",
    "    tie_token_emb = True      # tie embeddings of encoder and decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7dc0d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.639015Z",
     "start_time": "2021-11-01T13:09:14.599815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XTransformer(\n",
       "  (encoder): TransformerWrapper(\n",
       "    (token_emb): Embedding(256, 512)\n",
       "    (pos_emb): AbsolutePositionalEmbedding(\n",
       "      (emb): Embedding(1024, 512)\n",
       "    )\n",
       "    (emb_dropout): Dropout(p=0, inplace=False)\n",
       "    (project_emb): Identity()\n",
       "    (attn_layers): Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (6): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (7): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (8): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (9): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (10): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (11): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (to_logits): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): AutoregressiveWrapper(\n",
       "    (net): TransformerWrapper(\n",
       "      (token_emb): Embedding(256, 512)\n",
       "      (pos_emb): AbsolutePositionalEmbedding(\n",
       "        (emb): Embedding(1024, 512)\n",
       "      )\n",
       "      (emb_dropout): Dropout(p=0, inplace=False)\n",
       "      (project_emb): Identity()\n",
       "      (attn_layers): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (6): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (7): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (8): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (9): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (10): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (11): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (12): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (13): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (14): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (15): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (16): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (17): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (to_logits): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538fa780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.679223Z",
     "start_time": "2021-11-01T13:09:14.640799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45555200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_orig = get_parameter_count(model); params_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b4a9de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:09:14.721644Z",
     "start_time": "2021-11-01T13:09:14.680604Z"
    }
   },
   "outputs": [],
   "source": [
    "src = torch.randint(0, 256, (1, 1024))\n",
    "src_mask = torch.ones_like(src).bool()\n",
    "tgt = torch.randint(0, 256, (1, 1024))\n",
    "tgt_mask = torch.ones_like(tgt).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fef89fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:11:00.366700Z",
     "start_time": "2021-11-01T13:09:14.722994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.92 s Â± 1.25 s per loop (mean Â± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 3 loss = model(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask) # (1, 1024, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fdf6163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:30:22.062012Z",
     "start_time": "2021-11-01T13:30:21.685730Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_linear_factorization(model, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f16a019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:30:22.509870Z",
     "start_time": "2021-11-01T13:30:22.449868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XTransformer(\n",
       "  (encoder): TransformerWrapper(\n",
       "    (token_emb): Embedding(256, 512)\n",
       "    (pos_emb): AbsolutePositionalEmbedding(\n",
       "      (emb): Embedding(1024, 512)\n",
       "    )\n",
       "    (emb_dropout): Dropout(p=0, inplace=False)\n",
       "    (project_emb): Identity()\n",
       "    (attn_layers): Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_k): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_v): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                )\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_k): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_v): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                )\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_k): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_v): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                )\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (6): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_k): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_v): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (7): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                )\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (8): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_k): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_v): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (9): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                )\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (10): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_k): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (to_v): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (11): ModuleList(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                )\n",
       "                (1): GELU()\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (to_logits): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveWrapper(\n",
       "    (net): TransformerWrapper(\n",
       "      (token_emb): Embedding(256, 512)\n",
       "      (pos_emb): AbsolutePositionalEmbedding(\n",
       "        (emb): Embedding(1024, 512)\n",
       "      )\n",
       "      (emb_dropout): Dropout(p=0, inplace=False)\n",
       "      (project_emb): Identity()\n",
       "      (attn_layers): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (6): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (7): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (8): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (9): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (10): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (11): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (12): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (13): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (14): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (15): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (16): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_k): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (to_v): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (17): ModuleList(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=64, bias=True)\n",
       "                  (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (to_logits): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e75a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:30:23.178441Z",
     "start_time": "2021-11-01T13:30:23.117835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10035840"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_factorized = get_parameter_count(model); params_factorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd0e91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:30:23.686376Z",
     "start_time": "2021-11-01T13:30:23.641616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45555200, 10035840, 0.22)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_orig, params_factorized, round(params_factorized/params_orig, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "762cedff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T13:31:05.878078Z",
     "start_time": "2021-11-01T13:30:24.778883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.89 s Â± 194 ms per loop (mean Â± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 3 loss = model(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask) # (1, 1024, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e84e2a",
   "metadata": {},
   "source": [
    "**$\\rightarrow$ This setup saves ~70% of the parameters and is ~40% faster on CPU with this simple setup.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406adf4",
   "metadata": {},
   "source": [
    "# Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fe2da",
   "metadata": {},
   "source": [
    "1. Test the setup for standard Transformer setups!\n",
    "1. Combine this with reversible layers to decrease memory even more?\n",
    "1. Can the `rank` hyperparameter be learned, similar to [\"Adaptive Attention Span in Transformers\"](https://arxiv.org/abs/1905.07799) \n",
    "https://github.com/facebookresearch/adaptive-span/blob/main/adaptive_span.py)?\n",
    "1. Implement post-training low rank transformation with https://geotorch.readthedocs.io/en/latest/lowrank/lowrank.html?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.10",
   "language": "python",
   "name": "pytorch1.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
