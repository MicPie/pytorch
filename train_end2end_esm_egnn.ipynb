{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "curious-sense",
   "metadata": {},
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-coupon",
   "metadata": {},
   "source": [
    "Based on https://github.com/lucidrains/alphafold2 and https://github.com/lucidrains/egnn-pytorch with help from https://github.com/hypnopump."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-endorsement",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divided-advocacy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:36.776340Z",
     "start_time": "2021-03-27T20:18:36.423205Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-vatican",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compact-gates",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.190746Z",
     "start_time": "2021-03-27T20:18:36.777588Z"
    }
   },
   "outputs": [],
   "source": [
    "import sidechainnet as scn\n",
    "#from sidechainnet.sequence.utils import VOCAB\n",
    "from sidechainnet.utils.sequence import ProteinVocabulary as VOCAB # From https://github.com/lucidrains/egnn-pytorch/blob/main/examples/egnn_test.ipynb\n",
    "VOCAB = VOCAB()\n",
    "from sidechainnet.structure.build_info import NUM_COORDS_PER_RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italic-spyware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.198400Z",
     "start_time": "2021-03-27T20:18:37.192150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinVocabulary[size=21]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-split",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:34.529141Z",
     "start_time": "2021-03-27T11:16:34.449068Z"
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funny-tomato",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.259934Z",
     "start_time": "2021-03-27T20:18:37.199441Z"
    }
   },
   "outputs": [],
   "source": [
    "from alphafold2_pytorch import Alphafold2\n",
    "import alphafold2_pytorch.constants as constants\n",
    "\n",
    "from se3_transformer_pytorch import SE3Transformer\n",
    "from alphafold2_pytorch.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-newcastle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:34.606712Z",
     "start_time": "2021-03-27T11:16:34.599228Z"
    }
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "painted-rogers",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.263459Z",
     "start_time": "2021-03-27T20:18:37.261046Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURES = \"esm\" # one of [\"esm\", \"msa\", None]\n",
    "DEVICE = None # defaults to cuda if available, else cpu\n",
    "NUM_BATCHES = int(1e5)\n",
    "GRADIENT_ACCUMULATE_EVERY = 1 #16\n",
    "LEARNING_RATE = 3e-4\n",
    "IGNORE_INDEX = -100\n",
    "THRESHOLD_LENGTH = 250\n",
    "TO_PDB = False\n",
    "SAVE_DIR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reduced-criterion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.269741Z",
     "start_time": "2021-03-27T20:18:37.264403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('esm', None, 100000, 1, 0.0003, -100, 250, False, '')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES, DEVICE, NUM_BATCHES, GRADIENT_ACCUMULATE_EVERY, LEARNING_RATE, IGNORE_INDEX, THRESHOLD_LENGTH, TO_PDB, SAVE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-chicken",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:35.274958Z",
     "start_time": "2021-03-27T11:16:35.268764Z"
    }
   },
   "source": [
    "# Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "average-marks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.274239Z",
     "start_time": "2021-03-27T20:18:37.270715Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = constants.DEVICE\n",
    "DISTOGRAM_BUCKETS = constants.DISTOGRAM_BUCKETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forced-grass",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:37.280046Z",
     "start_time": "2021-03-27T20:18:37.275740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE, DISTOGRAM_BUCKETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-dinner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:44.738347Z",
     "start_time": "2021-03-27T11:16:35.993046Z"
    }
   },
   "source": [
    "# Set embedder model from esm if appropiate - Load ESM-1b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "varying-mathematics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:45.013919Z",
     "start_time": "2021-03-27T20:18:37.281114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mmp/.cache/torch/hub/facebookresearch_esm_master\n"
     ]
    }
   ],
   "source": [
    "if FEATURES == \"esm\":\n",
    "    # from pytorch hub (almost 30gb)\n",
    "    embedd_model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm1b_t33_650M_UR50S\")\n",
    "    ## Â alternatively do\n",
    "    # import esm # after installing esm\n",
    "    # model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "    batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-matter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:22:10.874606Z",
     "start_time": "2021-03-27T11:22:10.869292Z"
    }
   },
   "source": [
    "# AF2 helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aggregate-collectible",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:45.018800Z",
     "start_time": "2021-03-27T20:18:45.015031Z"
    }
   },
   "outputs": [],
   "source": [
    "def cycle(loader, cond = lambda x: True):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            if not cond(data):\n",
    "                continue\n",
    "            yield data\n",
    "\n",
    "def get_esm_embedd(seq):\n",
    "    str_seq = \"\".join([VOCAB.int2char(x) for x in seq.squeeze(0).cpu().numpy()])\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter( [(0, str_seq)] )\n",
    "    with torch.no_grad():\n",
    "        results = embedd_model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "    return results[\"representations\"][33].to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-power",
   "metadata": {},
   "source": [
    "https://github.com/jonathanking/sidechainnet#loading-sidechainnet-with-pytorch-dataloaders<br>\n",
    "`Downloaded SidechainNet to ./sidechainnet_data/sidechainnet_casp12_30.pkl.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-disposal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:48.960650Z",
     "start_time": "2021-03-27T11:16:44.744531Z"
    }
   },
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "capital-exchange",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:32:09.601275Z",
     "start_time": "2021-03-29T19:32:05.290229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SidechainNet was loaded from ./sidechainnet_data/sidechainnet_casp12_30.pkl.\n"
     ]
    }
   ],
   "source": [
    "data = scn.load(\n",
    "    casp_version = 12,\n",
    "    thinning = 30,\n",
    "    with_pytorch = 'dataloaders',\n",
    "    batch_size = 1,\n",
    "    dynamic_batching = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceramic-image",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:32:09.605675Z",
     "start_time": "2021-03-29T19:32:09.602649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'train-eval', 'test', 'valid-10', 'valid-20', 'valid-30', 'valid-40', 'valid-50', 'valid-70', 'valid-90'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "foster-literacy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:32:13.468004Z",
     "start_time": "2021-03-29T19:32:13.205433Z"
    }
   },
   "outputs": [],
   "source": [
    "data = iter(data['train'])\n",
    "data_cond = lambda t: t[1].shape[1] < THRESHOLD_LENGTH\n",
    "dl = cycle(data, data_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "incoming-syndication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:32:23.999347Z",
     "start_time": "2021-03-29T19:32:23.986370Z"
    }
   },
   "outputs": [],
   "source": [
    "#d_test = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "little-response",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:33:14.463530Z",
     "start_time": "2021-03-29T19:33:14.458150Z"
    }
   },
   "outputs": [],
   "source": [
    "#dir(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_test \"keys\": 'angs', 'count', 'crds', 'evos', 'index', 'int_seqs',\n",
    "#                'msks', 'pids', 'ress', 'secs', 'seq_evo_sec', 'seqs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-deployment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:54.778596Z",
     "start_time": "2021-03-27T11:16:49.083056Z"
    }
   },
   "source": [
    "# AF2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "placed-abortion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:52.684831Z",
     "start_time": "2021-03-27T20:18:49.332958Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Alphafold2(\n",
    "    dim = 128,\n",
    "    depth = 1,\n",
    "    heads = 1, # Maybe set even lower?\n",
    "    dim_head = 16, # Maybe set even lower?\n",
    "    predict_coords = False,\n",
    "    num_backbone_atoms = 3,\n",
    "    structure_module_dim = 8,\n",
    "    structure_module_depth = 2,\n",
    "    structure_module_heads = 4,\n",
    "    structure_module_dim_head = 16,\n",
    "    structure_module_refinement_iters = 2\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fancy-nightlife",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:52.691431Z",
     "start_time": "2021-03-27T20:18:52.687874Z"
    }
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-greenhouse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T11:16:54.795251Z",
     "start_time": "2021-03-27T11:16:54.782721Z"
    }
   },
   "source": [
    "# AF2 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "northern-horror",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:52.712666Z",
     "start_time": "2021-03-27T20:18:52.693459Z"
    }
   },
   "outputs": [],
   "source": [
    "dispersion_weight = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optim = Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-cambodia",
   "metadata": {},
   "source": [
    "# EGNN helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-radar",
   "metadata": {},
   "source": [
    "Based on: https://github.com/lucidrains/egnn-pytorch/blob/main/examples/egnn_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "insured-disposal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:44:45.853929Z",
     "start_time": "2021-03-29T19:44:45.844083Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_whole_protein(seq, true_coords, padding_seq,\n",
    "                         needed_info = { \"cutoffs\": [2, 5, 10],\n",
    "                                          \"bond_scales\": [0.5, 1, 2]}, free_mem=False):\n",
    "    \"\"\" Encodes a whole protein. In points + vectors. \"\"\"\n",
    "    device, precise = true_coords.device, true_coords.type()\n",
    "    #################\n",
    "    # encode points #\n",
    "    #################\n",
    "    cloud_mask = torch.tensor(scn_cloud_mask(seq[:-padding_seq or None])).bool().to(device)\n",
    "    flat_mask = rearrange(cloud_mask, 'l c -> (l c)')\n",
    "    coords_wrap = rearrange(true_coords, '(l c) d -> l c d', c=14)[:-padding_seq or None] \n",
    "    # embedd everything\n",
    "\n",
    "    # position in backbone embedding\n",
    "    aa_pos = encode_dist( torch.arange(len(seq[:-padding_seq or None]), device=device).float(), scales=needed_info[\"aa_pos_scales\"])\n",
    "    atom_pos = chain2atoms(aa_pos)[cloud_mask]\n",
    "\n",
    "    # atom identity embedding\n",
    "    atom_id_embedds = torch.stack([SUPREME_INFO[k][\"atom_id_embedd\"] for k in seq[:-padding_seq or None]], \n",
    "                                  dim=0)[cloud_mask].to(device)\n",
    "    # aa embedding\n",
    "    seq_int = torch.tensor([AAS2NUM[aa] for aa in seq[:-padding_seq or None]], device=device).long()\n",
    "    aa_id_embedds   = chain2atoms(seq_int, mask=cloud_mask)\n",
    "\n",
    "    ################\n",
    "    # encode bonds #\n",
    "    ################\n",
    "    bond_info = encode_whole_bonds(x = coords_wrap[cloud_mask],\n",
    "                                   x_format = \"coords\",\n",
    "                                   embedd_info = {},\n",
    "                                   needed_info = needed_info )\n",
    "    whole_bond_idxs, whole_bond_enc, bond_embedd_info = bond_info\n",
    "    #########\n",
    "    # merge #\n",
    "    #########\n",
    "\n",
    "    # concat so that final is [vector_dims, scalar_dims]\n",
    "    point_n_vectors = 0\n",
    "    point_n_scalars = 2*len(needed_info[\"aa_pos_scales\"]) + 1 +\\\n",
    "                      2 # the last 2 are to be embedded yet\n",
    "\n",
    "    whole_point_enc = torch.cat([ atom_pos, # 2n+1\n",
    "                                  atom_id_embedds.unsqueeze(-1),\n",
    "                                  aa_id_embedds.unsqueeze(-1) ], dim=-1) # the last 2 are yet to be embedded\n",
    "    if free_mem:\n",
    "        del cloud_mask, atom_pos, atom_id_embedds, aa_id_embedds\n",
    "\n",
    "    # record embedding dimensions\n",
    "    point_embedd_info = {\"point_n_vectors\": point_n_vectors,\n",
    "                         \"point_n_scalars\": point_n_scalars,}\n",
    "\n",
    "    embedd_info = {**point_embedd_info, **bond_embedd_info}\n",
    "\n",
    "    return whole_point_enc, whole_bond_idxs, whole_bond_enc, embedd_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "possible-stocks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:44:46.214650Z",
     "start_time": "2021-03-29T19:44:46.204798Z"
    }
   },
   "outputs": [],
   "source": [
    "NEEDED_INFO = {\"cutoffs\": [], # \"15_closest\"\n",
    "               \"bond_scales\": [1, 2],\n",
    "               \"aa_pos_scales\": [2,4,8,16,32,64,128],\n",
    "               \"atom_pos_scales\": [1,2,4,8,16,32],\n",
    "               \"dist2ca_norm_scales\": [1,2,4],\n",
    "               \"bb_norms_atoms\": [0.5], # will encode 3 vectors with this\n",
    "               # nn-degree connection\n",
    "               \"adj_degree\": 2\n",
    "              }\n",
    "# get model sizes from encoded protein\n",
    "#seq, true_coords, angles, padding_seq, mask, id = train_examples_storer[-1] \n",
    "#NEEDED_INFO[\"seq\"] = seq[:-padding_seq or None]\n",
    "#NEEDED_INFO[\"covalent_bond\"] = prot_covalent_bond(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "elder-papua",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:44:46.642130Z",
     "start_time": "2021-03-29T19:44:46.637123Z"
    }
   },
   "outputs": [],
   "source": [
    "### adjust for egnn: \n",
    "#embedd_info[\"bond_n_scalars\"] -= 2*len(NEEDED_INFO[\"bond_scales\"])+1\n",
    "#embedd_info[\"bond_n_vectors\"] = 0\n",
    "#embedd_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "expensive-advance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:44:47.143414Z",
     "start_time": "2021-03-29T19:44:47.135045Z"
    }
   },
   "outputs": [],
   "source": [
    "def prot_covalent_bond(seq, adj_degree=1, cloud_mask=None):\n",
    "    \"\"\" Returns the idxs of covalent bonds for a protein.\n",
    "        Inputs \n",
    "        * seq: str. Protein sequence in 1-letter AA code.\n",
    "        * cloud_mask: mask selecting the present atoms.\n",
    "        Outputs: edge_idxs\n",
    "    \"\"\"\n",
    "    # create or infer cloud_mask\n",
    "    if cloud_mask is None: \n",
    "        cloud_mask = scn_cloud_mask(seq).bool()\n",
    "    device, precise = cloud_mask.device, cloud_mask.type()\n",
    "    # get starting poses for every aa\n",
    "    scaff = torch.zeros_like(cloud_mask)\n",
    "    scaff[:, 0] = 1\n",
    "    idxs = scaff[cloud_mask].nonzero().view(-1)\n",
    "    # get poses + idxs from the dict with GVP_DATA - return all edges\n",
    "    adj_mat = torch.zeros(idxs.amax()+14, idxs.amax()+14)\n",
    "    attr_mat = torch.zeros_like(adj_mat)\n",
    "    for i,idx in enumerate(idxs):\n",
    "        # bond with next aa\n",
    "        extra = []\n",
    "        if i < idxs.shape[0]-1:\n",
    "            extra = [[2, (idxs[i+1]-idx).item()]]\n",
    "\n",
    "        bonds = idx + torch.tensor( GVP_DATA[seq[i]]['bonds'] + extra ).long().t() \n",
    "        adj_mat[bonds[0], bonds[1]] = 1.\n",
    "\n",
    "    # convert to undirected\n",
    "    adj_mat = adj_mat + adj_mat.t()\n",
    "    # do N_th degree adjacency\n",
    "    for i in range(adj_degree):\n",
    "        if i == 0:\n",
    "            attr_mat += adj_mat\n",
    "            continue\n",
    "\n",
    "        adj_mat = (adj_mat @ adj_mat).bool().float() \n",
    "        attr_mat[ (adj_mat - attr_mat.bool().float()).bool() ] += i+1\n",
    "\n",
    "    edge_idxs = attr_mat.nonzero().t().long()\n",
    "    edge_attrs = attr_mat[edge_idxs[0], edge_idxs[1]]\n",
    "\n",
    "    return edge_idxs, edge_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-diameter",
   "metadata": {},
   "source": [
    "# EGNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "from egnn_pytorch import EGNN_Sparse_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-coordinator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:44:48.058418Z",
     "start_time": "2021-03-29T19:44:48.046949Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model_egnn = EGNN_Sparse_Network(n_layers=4,\n",
    "                                 feats_dim=2, pos_dim = 3,\n",
    "                                 edge_attr_dim = 1, m_dim = 32,\n",
    "                                 fourier_features = 4,\n",
    "                                 embedding_nums=[36,20], embedding_dims=[16,16],\n",
    "                                 edge_embedding_nums=[3], edge_embedding_dims=[2],\n",
    "                                 update_coors=True, update_feats=True, \n",
    "                                 norm_feats=False, norm_coors=False, recalc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_egnn = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1\n",
    "optimizer = torch.optim.Adam(model_egnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-officer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T19:19:56.343261Z",
     "start_time": "2021-03-27T19:19:55.635175Z"
    }
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-philip",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T19:43:40.815993Z",
     "start_time": "2021-03-29T19:43:40.256466Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(NUM_BATCHES):\n",
    "    for _ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        \n",
    "        ### Stage 1\n",
    "        \n",
    "        batch = next(dl)\n",
    "        seq, coords, mask = batch.seqs, batch.crds, batch.msks\n",
    "        mask = mask.bool() # Needs to be set to bool\n",
    "\n",
    "        b, l, _ = seq.shape\n",
    "\n",
    "        # prepare data and mask labels\n",
    "        seq, coords, mask = seq.argmax(dim = -1).to(DEVICE), coords.to(DEVICE), mask.to(DEVICE)\n",
    "        #Â coords = rearrange(coords, 'b (l c) d -> b l c d', l = l) # no need to rearrange for now\n",
    "        # mask the atoms and backbone positions for each residue\n",
    "\n",
    "        # sequence embedding (msa / esm / attn / or nothing)\n",
    "        msa, embedds = None, None\n",
    "\n",
    "        #Â get embedds\n",
    "        if FEATURES == \"esm\":\n",
    "            #embedds = get_esm_embedd(seq)\n",
    "            embedds = get_esm_embedd(seq).unsqueeze(0)\n",
    "            msa_mask = None\n",
    "            #msa_mask = torch.ones_like(embedds).bool()\n",
    "            #msa_mask = torch.ones_like(embedds[..., -1]).bool()\n",
    "        # get msa here\n",
    "        elif FEATURES == \"msa\":\n",
    "            pass\n",
    "        # no embeddings\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # predict - out isÂ (batch, L * 3, 3)\n",
    "\n",
    "        refined = model(\n",
    "            seq,\n",
    "            msa = msa,\n",
    "            embedds = embedds,\n",
    "            mask = mask,\n",
    "            msa_mask = msa_mask\n",
    "            )\n",
    "        \n",
    "        ### Stage 2\n",
    "        \n",
    "        distance_pred = refined # is this correct?\n",
    "        \n",
    "        \n",
    "        # prepare mask for backbone coordinates\n",
    "\n",
    "        assert model.num_backbone_atoms > 1, 'must constitute to at least 3 atomic coordinates for backbone'\n",
    "\n",
    "        N_mask, CA_mask, C_mask = scn_backbone_mask(seq, boolean = True, n_aa = model.num_backbone_atoms)\n",
    "\n",
    "        cloud_mask = scn_cloud_mask(seq, boolean=True)\n",
    "        flat_cloud_mask = rearrange(cloud_mask, 'b l c -> b (l c)')\n",
    "        chain_mask = (mask.unsqueeze(-1) * cloud_mask)\n",
    "        flat_chain_mask = rearrange(chain_mask, 'b l c -> b (l c)')\n",
    "\n",
    "        bb_mask = rearrange(chain_mask[:, :, :model.num_backbone_atoms], 'b l c -> b (l c)')\n",
    "        bb_mask_crossed = rearrange(bb_mask, 'b i -> b i ()') * rearrange(bb_mask, 'b j -> b () j')\n",
    "\n",
    "        # structural refinement\n",
    "\n",
    "        if model.predict_real_value_distances:\n",
    "            distances, distance_std = distance_pred.unbind(dim = -1)\n",
    "            weights = (1 / (1 + distance_std)) # could also do a distance_std.sigmoid() here\n",
    "        else:\n",
    "            distances, weights = center_distogram_torch(distance_pred)\n",
    "\n",
    "        weights.masked_fill_(bb_mask_crossed, 0.)\n",
    "\n",
    "        coords_3d, _ = MDScaling(distances, \n",
    "            weights = weights,\n",
    "            iters = model.mds_iters,\n",
    "            fix_mirror = True,\n",
    "            N_mask = N_mask,\n",
    "            CA_mask = CA_mask,\n",
    "            C_mask = C_mask\n",
    "        )\n",
    "        coords = rearrange(coords_3d, 'b c n -> b n c')\n",
    "        \n",
    "        ### Stage 3\n",
    "        \n",
    "        # See below for code from EGNN loop:\n",
    "        ## encode as needed\n",
    "        #encoded = encode_whole_protein(seq, true_coords, padding_seq, needed_info=NEEDED_INFO, free_mem=True)\n",
    "        #x, edge_index, edge_attrs, embedd_info = encoded\n",
    "        ## add position coords\n",
    "        #cloud_mask = scn_cloud_mask(seq)\n",
    "        #if padding_seq:\n",
    "        #    cloud_mask[-padding_seq:] = 0.\n",
    "        #cloud_mask = cloud_mask.bool()\n",
    "        #flat_cloud_mask = rearrange(cloud_mask, 'l c -> (l c)')\n",
    "        #x = torch.cat([true_coords[flat_cloud_mask], x ], dim=-1)\n",
    "        \n",
    "        \n",
    "        # We need as shown in line:\n",
    "        # seq, true_coords, angles, padding_seq, mask, pid = get_prot(dataloader_=dataloaders_,\n",
    "        \n",
    "        # From batch \"keys\":\n",
    "        # 'angs', 'count', 'crds', 'evos', 'index', 'int_seqs', 'msks', 'pids', 'ress', 'secs', 'seq_evo_sec', 'seqs'\n",
    "        \n",
    "        # seq: take from above\n",
    "        true_coords = coords\n",
    "        angles = batch.angs.to(DEVICE)\n",
    "        # padding_seq ?\n",
    "        mask = batch.msks.to(DEVICE)\n",
    "        pid = batch.pids #.to(DEVICE)\n",
    "        \n",
    "        # encode as needed\n",
    "        masked_coords = true_coords + noise * torch.randn_like(true_coords) # (*2-1)\n",
    "        encoded = encode_whole_protein(seq, true_coords, padding_seq, needed_info=NEEDED_INFO, free_mem=True)\n",
    "        x, edge_index, edge_attrs, embedd_info = encoded\n",
    "        \n",
    "        # add position coords - better mask accounting for missing atoms\n",
    "        cloud_mask_naive = scn_cloud_mask(seq).bool()\n",
    "        cloud_mask = scn_cloud_mask(seq, coords=true_coords).bool()\n",
    "        if padding_seq:\n",
    "            cloud_mask[-padding_seq:] = 0.\n",
    "        # cloud is all points, chain is all for which we have labels\n",
    "        chain_mask = mask.unsqueeze(-1) * cloud_mask\n",
    "        flat_chain_mask = rearrange(chain_mask, 'l c -> (l c)')\n",
    "        flat_cloud_mask = rearrange(cloud_mask, 'l c -> (l c)')\n",
    "        # slice useless norm and vector embeddings\n",
    "        masked_coords = masked_coords[flat_cloud_mask]\n",
    "\n",
    "        #############\n",
    "        # MASK EDGES AND NODES ACCOUNTING FOR SCN MISSING ATOMS\n",
    "        #############\n",
    "        # NODES\n",
    "        x = torch.cat([masked_coords, x[:, -2:][cloud_mask[cloud_mask_naive]] ], dim=-1)\n",
    "        # EDGES: delete all edges with masked-out atoms\n",
    "\n",
    "        # pick all current indexes and turn them to 1.\n",
    "        to_mask_edges = torch.zeros(edge_index.amax()+1, edge_index.amax()+1).to(edge_index.device)\n",
    "        to_mask_edges[edge_index[0], edge_index[1]] = 1.\n",
    "        # delete erased bonds\n",
    "        masked_out_atoms = (-1*(cloud_mask[cloud_mask_naive].float() - 1)).bool()\n",
    "        to_mask_edges[masked_out_atoms] *= 0.\n",
    "        to_mask_edges = to_mask_edges * to_mask_edges.t()\n",
    "        # get mask for the edge_attrs\n",
    "        attr_mask = to_mask_edges[edge_index[0], edge_index[1]].bool()\n",
    "        edge_attrs = edge_attrs[attr_mask, :]\n",
    "        # delete unwanted rows and cols\n",
    "        wanted = to_mask_edges.sum(dim=-1).bool()\n",
    "        edge_index = (to_mask_edges[wanted, :][:, wanted]).nonzero().t()\n",
    "        #############\n",
    "        # continue\n",
    "        #############\n",
    "        edge_attrs = edge_attrs[:, -1:]\n",
    "        batch = torch.tensor([0 for i in range(x.shape[0])], device=device).long()\n",
    "\n",
    "        if torch.amax(edge_index) >= x.shape[0]:\n",
    "            print(\"wtf, breaking, debug, index out of bounds\")\n",
    "            break\n",
    "\n",
    "        # predict\n",
    "        preds = model.forward(x, edge_index, batch=batch, edge_attr=edge_attrs,\n",
    "                              recalc_edge=None, verbose = False)\n",
    "\n",
    "        # MEASURE ERROR - format pred and target\n",
    "        target_coords = true_coords[flat_cloud_mask].clone()\n",
    "        pred_coords   = preds[:, :3]\n",
    "        base_coords   = x[:, :3]\n",
    "\n",
    "        # option 2: loss is RMSD on reconstructed coords  // align - sometimes svc fails - idk why\n",
    "        try:\n",
    "            pred_aligned, target_aligned = kabsch_torch(pred_coords.t(), target_coords.t()) # (3, N)\n",
    "\n",
    "            loss = ( (pred_aligned.t() - target_aligned.t())[flat_chain_mask[flat_cloud_mask]]**2 ).mean() \n",
    "        except:\n",
    "            pred_aligned, target_aligned = None, None\n",
    "            print(\"svd failed convergence, ep:\", ep)\n",
    "            loss = ( (pred_coords - target_coords)[flat_chain_mask[flat_cloud_mask]]**2 ).mean()\n",
    "        # measure error\n",
    "        loss_base = ((base_coords - target_coords)**2).mean() \n",
    "        # not aligned: # loss = ((pred_coords - target_coords)**2).mean()**0.5 \n",
    "\n",
    "        # back pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # records / prints\n",
    "        iteration += 1\n",
    "        epoch_losses.append( loss.item() )\n",
    "        baseline_losses.append( loss_base.item() )\n",
    "\n",
    "        n_print = 10\n",
    "        if iteration % n_print == 1:\n",
    "            tic = time.time()\n",
    "            print(\"BATCH: {0} / {1}, loss: {2}, baseline_loss: {3}, time: {4}\".format(iteration, n_per_iter,\n",
    "                                                                                      np.mean(epoch_losses[-n_print:]),\n",
    "                                                                                      baseline_losses[-1],\n",
    "                                                                                      tic-tac))\n",
    "            tac = time.time()\n",
    "            if iteration % n_per_iter == 1:\n",
    "                print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-growth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:18:53.053571Z",
     "start_time": "2021-03-27T20:18:52.714511Z"
    }
   },
   "outputs": [],
   "source": [
    "        ### Old stuff:\n",
    "\n",
    "#        # build SC container. set SC points to CA and optionally place carbonyl O\n",
    "#        proto_sidechain = sidechain_container(refined, n_aa=batch,\n",
    "#                                              cloud_mask=cloud_mask, place_oxygen=False)\n",
    "#\n",
    "#        # rotate / align\n",
    "#        coords_aligned, labels_aligned = Kabsch(refined, coords[flat_cloud_mask])\n",
    "#\n",
    "#        # atom mask\n",
    "#\n",
    "#        cloud_mask = scn_cloud_mask(seq, boolean = False)\n",
    "#        flat_cloud_mask = rearrange(cloud_mask, 'b l c -> b (l c)')\n",
    "#\n",
    "#        #Â chain_mask is all atoms that will be backpropped thru -> existing + trainable\n",
    "#\n",
    "#        chain_mask = (mask * cloud_mask)[cloud_mask]\n",
    "#        flat_chain_mask = rearrange(chain_mask, 'b l c -> b (l c)')\n",
    "#\n",
    "#        # save pdb files for visualization\n",
    "#\n",
    "#        if TO_PDB:\n",
    "#            # idx from batch to save prot and label\n",
    "#            idx = 0\n",
    "#            coords2pdb(seq[idx, :, 0], coords_aligned[idx], cloud_mask, prefix=SAVE_DIR, name=\"pred.pdb\")\n",
    "#            coords2pdb(seq[idx, :, 0], labels_aligned[idx], cloud_mask, prefix=SAVE_DIR, name=\"label.pdb\")\n",
    "#\n",
    "#        # loss - RMSE + distogram_dispersion\n",
    "#        loss = torch.sqrt(criterion(coords_aligned[flat_chain_mask], labels_aligned[flat_chain_mask])) + \\\n",
    "#                          dispersion_weight * torch.norm( (1/weights)-1 )\n",
    "#\n",
    "#        loss.backward()\n",
    "#    print('loss:', loss.item())\n",
    "#\n",
    "#    optim.step()\n",
    "#    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-provincial",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-destiny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold2_pytorch",
   "language": "python",
   "name": "alphafold2_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
